# -*- coding: utf-8 -*-
"""naive_bayes_for_sentiment_ananlysis

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yLSdH9zyc7yckVcO2bzQYrMRSS5POh4F

##**Naive Bayes classifier**
In this colab we will apply the naive bayes classifier to the sentimaent analysis for the given text

##Importing Libraries
"""

import numpy as np
import matplotlib.pyplot as plt

from sklearn.metrics import confusion_matrix

"""##Sentiment Analysis
In the case of sentiment we are required to find out the sentiment in the given text.This exercise is only is for getting acquaintance with one of the application of naive bayes in real world dataset.

Here all the preprocessing has been done already.

In the below code snippet we have mounted content of google drive in the google colab.\
Now you can save the given .txt file in google drive and read store it
"""

from google.colab import drive
drive.mount('/content/drive')

"""##Bayes Classifier

Bayes classifiers fall under the class of generative classifiers. Generative classifiers attempt to learn the generation process of a dataset, usually by making some assumptions about the process that generates the data. Then such classifiers use the learned model to make a prediction or classify the unseen data. A simple example is a Na誰ve Bayes Classifier.

### Na誰ve Bayes classifier

Consider a dataset $\left\{X^{(i)}, Y^{(i)}\right\}_{i=1}^{m}$. Each $X^{(i)}$ is an $n-$dimensional vector of input features. Let $Y^{(i)} \in \{0,1\}$ denote the class to which $X^{(i)}$ belongs (this can be easily extended to multi-class problems as well). A good classifier has to accurately predict the probability that any given input $X$ falls in class $1$ which is $ P(Y=1 | X)$. 

Recall Bayes theorem,

\begin{align}
P(Y|X) &= \frac{P(X|Y)P(Y)}{P(X)} \\
       &= \frac{P(X_1, X_2, \dots, X_n | Y)P(Y)}{P(X_1, X_2, \dots, X_n)}\\
\end{align}

**We use the assumption that features are independent of each other. That is one particular feature does not affect any other feature. Of course these assumptions of independence are rarely true, which is why the model is referred as the "Na誰ve Bayes" model. However, in practice, Na誰ve Bayes models have performed surprisingly well even on complex tasks, where it is clear that the strong independence assumptions are false.**

The independence assumption reduces the conditional probability expression to
\begin{align}
P(Y|X) &= \frac{P(X_1 | Y)P(X_2 | Y) \dots P(X_n | Y)P(Y)}{P(X_1)P(X_2)\dots P(X_n)}\\
\end{align}

The terms $P(X_i|Y)$ and $P(X_i)$ can be easily estimated/learned from the dataset. Hence, the value of $P(Y|X)$ can be found for each value of $Y$. Finally, the class to which $X$ belongs is estimated as $arg\max_{Y}P(Y|X)$. Moreover since $X$ is independent of $Y$, it is only required to find $arg\max_{Y}P(X|Y)P(Y).$
"""

def read_corpus(corpus_file):
    """ This function reads the file in the location specified by string 
    `corpus_file` and returns a list of tuples (list of words in text, label)
    """
    out = []
    with open(corpus_file) as f:
        for line in f:
            tokens = line.strip().split()
            out.append((tokens[3:], tokens[1]))
    return out

corpus = read_corpus('read the text document here') #Here '/content/drive/My Drive/Location where your file is saved' 
print("Example:\n", " Text: ", corpus[0][0], "\n  Label: ", corpus[0][1])
print("Total number of documents =", len(corpus))

"""### Preprocessing a text document
We can guess that not all the words in a document will be helpful in classification. The words such as "a", "the", "is", etc appear in all the documents randomly and can be neglected or removed. Also a same word can be written in different tenses while conveying the same mood (example "rot"/"rotten"). Hence the documents need to be preprocessed before using them for training the classifier.

 Libraries such as `gensim`, `nltk` contain functions for doing these preprocessing steps .Formally, these are the preprocessings to be done to the input text to make them simpler and which can improve the performance of your model as well.
* **Tokenization**: 
    1.   Split the text into sentences and the sentences into words
    2.   Lowercase the words and remove punctuation
* Remove all **stopwords** (stopwords are commonly used word such as "the", "a", "an", "in")
* Remove all words that have fewer than 3 characters.
* **Lemmatize** the document (words in third person are changed to first person, and verbs in past and future tenses are changed into present).




"""

##Here we have imported nltk library (nltk is  mainly used for natural language processing )
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from collections import Counter 
lemmatizer=WordNetLemmatizer()
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

""" Implement preprocessing functions here. Use the python modules named above 
for implementing the functions. 
"""

# Removes all the punctuations present in the document
def remove_punctuation(info):
    # implement
    punctuation=[]
    for word in info[:][0]:
      if word.isalpha():
        low=word.lower()
        punctuation.append(low)
    punctuation_rem=(punctuation,info[:][1])
     
    
    # comment the next line out
     #pass
    return punctuation_rem

# Removes words like 'if', 'he', 'she', 'the', etc which never belongs to any topic
def remove_stopwords(doc):
    # implement
    stop_words=set(stopwords.words('english'))
    filter_sentence=[]
    for word in doc[:][0]:
      if not word in stop_words:  
        filter_sentence.append(word)
    filter_tup=(filter_sentence,doc[:][1])     
    # comment the next line out
   #  pass
    return filter_tup
# lemmatizer is a transformers which transforms the word to its singular, present-tense form
def lemmatize(doc):
    # implement
    lemmatized_word=[]
    for word in doc[:][0]:
      lem_word=lemmatizer.lemmatize(word,pos='v')
      lem_word=lemmatizer.lemmatize(lem_word,pos='n')
      lem_word=lemmatizer.lemmatize(lem_word,pos='a')
      lem_word=lemmatizer.lemmatize(lem_word,pos='r')
      lemmatized_word.append(lem_word)
    lem=(lemmatized_word,doc[:][1])

    return lem

def preprocess(doc):
    """ Function to preprocess a single document
    """
    assert isinstance(doc, str) # assert that input is a document and not the corpus
    data=read_corpus(doc)
    #print(data[:10][0])
    p=[]
    for i in range(len(data)):
      preprocessed_doc = remove_punctuation(data[i][:])
      preprocessed_doc = remove_stopwords(preprocessed_doc)

      preprocessed_doc = lemmatize(preprocessed_doc)
      p.append(preprocessed_doc)
    return p

dat=preprocess('/content/drive/My Drive/all_sentiment_shuffled.txt')

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import CountVectorizer

#Since the label "pos" and "neg " is given as the last element in the given array of words which is 'dat' here
def label(x):
  out=[]
  for i in range(len(x)):
    if x[i][-1]=='neg':
      out+=[0]
    if x[i][-1]=='pos':
      out+=[1]
  return out

y=label(dat) #We have collected all labels in given y

"""After the required preprocessing which is done in earlier snipets we get the line which is splitted in the words form.

But the sklearn feature extractor required whole sentence  as elements of array.
So by using below function we will store the sentences as element of the array.
Now the given array will be paassed through countvectors where it is prepare as the input which can we give to the .fit of naive bayes
"""

def join(data):
  array_sentence=[]
  for i in range(len(dat)):
    array_sentence.append(' '.join(dat[i][0]))
  return array_sentence
out=join(dat)

from sklearn.feature_extraction.text import CountVectorizer
vect = CountVectorizer()
vect.fit(out)
dat_dtm = vect.transform(out)

X_train,X_test,y_train,y_test = train_test_split(dat_dtm,y, test_size=0.2, random_state=10)

from sklearn.naive_bayes import MultinomialNB

mnb = MultinomialNB()
mnb.fit(X_train, y_train)
y_test_pred=mnb.predict(X_test)
accuracy=mnb.score(X_test,y_test)
cm=confusion_matrix(y_test,y_test_pred)

print('Confusion matrix for multinomial Naive Bayes\n',cm)
print("accuracy for multinomial Naive bayes is {}".format(accuracy*100))